{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9ec1e3c",
   "metadata": {},
   "source": [
    "# Using and understanding Catalogs\n",
    "\n",
    "<div class=\"alert alert-info\"> <b>NOTE:</b> Catalogs in `xscen` are built upon Datastores in `intake_esm`. For more information on basic usage, such as the `search()` function, please consult their documentation: <a href=\"https://intake-esm.readthedocs.io/en/stable/\">https://intake-esm.readthedocs.io/en/stable/</a>.</div>\n",
    "\n",
    "Catalogs are made of two files:\n",
    "\n",
    "- JSON file containing metadata such as the catalog's title, description etc. It also contains an attribute *catalog_file* that points towards the CSV.\n",
    "- CSV file containing the catalog itself. This file can be zipped.\n",
    "\n",
    "Two types of catalogs have been implemented in `xscen`.\n",
    "\n",
    "- __Static catalogs:__ A `DataCatalog` is a *read-only* `intake-esm` catalog that contains information on all available data. Usually, this type of catalog should only be consulted at the start of a new project.\n",
    "\n",
    "- __Updatable catalogs:__ A `ProjectCatalog` is a *DataCatalog* with additional *write* functionalities. This kind of catalog should be used to keep track of the new data created during the course of a project, such as regridded or bias-corrected data, since it can `update` itself and append new information to the associated CSV file.\n",
    "\n",
    "__NOTE:__ As to not accidentaly lose data, both catalogs currently have no function to remove data from the CSV file. However, upon initialisation and when updating or refreshing itself, the catalog validates that all entries still exist and, if files have been manually removed, deletes their entries from the catalog.\n",
    "\n",
    "Catalogs in `xscen` are made to follow a nomenclature that is as close as possible to the Python Earth Science Standard Vocabulary : [https://github.com/ES-DOC/pyessv](https://github.com/ES-DOC/pyessv). The columns are listed below but for more details and concrete examples about the entries, consult [the relevant page in the documentation](../columns.rst):\n",
    "\n",
    "| Column name | Description |\n",
    "| :- | :- |\n",
    "| id | Unique DatasetID generated by `xscen` based on a subset of columns. |\n",
    "| type | Type of data: [forecast, station-obs, gridded-obs, reconstruction, simulation] |\n",
    "| processing_level | Level of post-processing reached: [raw, extracted, regridded, biasadjusted] |\n",
    "| bias_adjust_institution | Institution that computed the bias adjustment. |\n",
    "| bias_adjust_project | Name of the project that computed the bias adjustment. |\n",
    "| mip_era | CMIP Generation associated with the data. |\n",
    "| activity | Model Intercomparison Project (MIP) associated with the data. |\n",
    "| driving_institution | Institution of the driver model. |\n",
    "| driving_model | Name of the driver. |\n",
    "| institution | Institution associated with the source. |\n",
    "| source | Name of the model or the dataset. |\n",
    "| experiment | Name of the experiment of the model. |\n",
    "| member | Name of the realisation (or of the driving realisation in the case of RCMs). |\n",
    "| xrfreq | Pandas/xarray frequency. |\n",
    "| frequency | Frequency in letters (CMIP6 format). |\n",
    "| variable | Variable(s) in the dataset. |\n",
    "| domain | Name of the region covered by the dataset. |\n",
    "| date_start | First date of the dataset. |\n",
    "| date_end | Last date of the dataset. |\n",
    "| version | Version of the dataset. |\n",
    "| format | Format of the dataset. |\n",
    "| path | Path to the dataset. |\n",
    "\n",
    "Individual projects may use a different set of columns, but those will always be present in the official Ouranos internal catalogs. Some parts of `xscen` will however expect certain column names, so diverging from the official list is to be done with care.\n",
    "\n",
    "## Basic Catalog Usage\n",
    "\n",
    "If an official catalog already exists, it should be opened using `xs.DataCatalog` by pointing it to the JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25ee70d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from xscen import DataCatalog, ProjectCatalog\n",
    "\n",
    "output_folder = Path().absolute() / \"_data\"\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "DC = DataCatalog(f\"{Path().absolute()}/samples/pangeo-cmip6.json\")\n",
    "\n",
    "DC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47411b46",
   "metadata": {},
   "source": [
    "The content of the catalog can be accessed by a call to `df`, which will return a `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3840b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the catalog\n",
    "DC.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28d8c93",
   "metadata": {},
   "source": [
    "The `unique` function allows listing unique elements for either all the catalog or a subset of columns. It can be called in a few various ways, listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d04a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all unique elements in the catalog, returns a pandas.Series\n",
    "DC.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f67f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all unique elements in a subset of columns, returns a pandas.Series\n",
    "DC.unique([\"variable\", \"frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef86a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all unique elements in a single columns, returns a list\n",
    "DC.unique(\"id\")[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5c42f5",
   "metadata": {},
   "source": [
    "### Basic .search() commands\n",
    "\n",
    "The `search` function comes from `intake-esm` and allows searching for specific elements in the catalog's columns. It accepts both wildcards and regular expressions (except for *variable*, which must be exact due to being in *tuples*).\n",
    "\n",
    "While regex isn't great at inverse matching (\"does not contain\"), it is possible. Here are a few useful commands:\n",
    "\n",
    "    - ^string            : Starts with string\n",
    "\n",
    "    - string$            : Ends with string\n",
    "\n",
    "    - ^(?!string).*$     : Does not start with string\n",
    "\n",
    "    - .*(?<!string)$     : Does not end with string\n",
    "\n",
    "    - ^((?!string).)*$   : Does not contain substring\n",
    "\n",
    "    - ^(?!string$).*$    : Is not that exact string\n",
    "\n",
    "This website can be used to test regex commands: https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67815ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex: Find all entries that start with \"rcp\"\n",
    "print(DC.search(experiment=\"^ssp\").unique(\"experiment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7b426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex: Exclude all entries that start with \"ssp\"\n",
    "print(DC.search(experiment=\"^(?!ssp).*$\").unique(\"experiment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45af639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex: Find all experiments except the exact string \"ssp245\"\n",
    "print(DC.search(experiment=\"^(?!ssp245$).*$\").unique(\"experiment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa5aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wildcard: Find all entries that start with NorESM2\n",
    "print(DC.search(source=\"NorESM2.*\").unique(\"source\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f27214c-2524-4fb7-9b95-4a384ed13a53",
   "metadata": {},
   "source": [
    "It is also possible to search for files that intersect a specific time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda16b2-2d87-495d-b1e1-c2894b83fb1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DC.search(periods=[[\"2016\", \"2017\"]]).unique([\"date_start\", \"date_end\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa903092",
   "metadata": {},
   "source": [
    "### Advanced search: xs.search_data_catalogs\n",
    "\n",
    "`search` has multiple notable limitations for more advanced searches:\n",
    "\n",
    "- It can't match specific criteria together, such as finding a dataset that would have both 3h precipitation and daily temperature.\n",
    "- It has no explicit understanding of climate datasets, and thus can't match historial and future simulations together or know how realization members or grid resolutions work.\n",
    "\n",
    "`xs.search_data_catalogs` was thus created as a more advanced version that is closer to the needs of climate services. It also plays the double role of preparing certain arguments for the extraction function.\n",
    "\n",
    "Due to how different reference datasets are from climate simulations, this function might have to be called multiple times and the results concatenated into a single dictionary. The main arguments are:\n",
    "\n",
    "- `variables_and_freqs` is used to indicate which variable and which frequency is required. <b> NOTE:</b> With the exception of fixed fields, where *'fx'* should be used, frequencies here use the `pandas` nomenclature ('D', 'H', '6H', 'MS', etc.).\n",
    "- `other_search_criteria` is used to search for specific entries in other columns of the catalog, such as *activity*.\n",
    "- `exclusions` is used to exclude certain simulations or keywords from the results.\n",
    "- `match_hist_and_fut` is used to indicate that RCP/SSP simulations should be matched with their *historical* counterparts.\n",
    "- `periods` is used to search for specific time periods.\n",
    "- `allow_resampling` is used to allow searching for data at higher frequencies than requested.\n",
    "- `allow_conversion` is used to allow searching for calculable variables, in the case where the requested variable would not be available.\n",
    "- `restrict_resolution` is used to limit the results to the finest or coarsest resolution available for each source.\n",
    "- `restrict_members` is used to limit the results to a maximum number of realizations for each source.\n",
    "- `restrict_warming_level` is used to limit the results to only datasets that are present in the csv used for calculating warming levels.\n",
    "\n",
    "Note that compared to `search`, the result of `search_data_catalog` is a dictionary with one entry per unique ID. A given unique ID might contain multiple datasets as per `intake-esm`'s definition, because it groups catalog lines per *id - domain - processing_level - xrfreq*. Thus, it separates model data that exists at different frequencies.\n",
    "\n",
    "\n",
    "#### Example 1: Simple dataset\n",
    "\n",
    "Let's start by searching for CMIP6 data that has subdaily precipitation, daily temperature and the land fraction data. The main difference compared to searching for reference datasets is that in most cases, `match_hist_and_fut` will be required to match *historical* simulations to their future counterparts. This works for both CMIP5 and CMIP6 nomenclatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d91cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xscen as xs\n",
    "\n",
    "variables_and_freqs = {\"tas\": \"D\", \"pr\": \"3H\", \"sftlf\": \"fx\"}\n",
    "other_search_criteria = {\"institution\": [\"NOAA-GFDL\"], \"experiment\": [\"ssp585\"]}\n",
    "\n",
    "cat_sim = xs.search_data_catalogs(\n",
    "    data_catalogs=[f\"{Path().absolute()}/samples/pangeo-cmip6.json\"],\n",
    "    variables_and_freqs=variables_and_freqs,\n",
    "    other_search_criteria=other_search_criteria,\n",
    "    match_hist_and_fut=True,\n",
    ")\n",
    "\n",
    "cat_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ee34fe",
   "metadata": {},
   "source": [
    "Two simulations correspond to the search criteria, but as can be seen from the results, it is the same simulation on 2 different grids (`gr1` and `gr2`). If desired, `restrict_resolution` can be called to choose the finest or coarsest grid in such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1958d406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variables_and_freqs = {\"tas\": \"D\", \"pr\": \"3H\", \"sftlf\": \"fx\"}\n",
    "other_search_criteria = {\"institution\": [\"NOAA-GFDL\"], \"experiment\": [\"ssp585\"]}\n",
    "\n",
    "cat_sim = xs.search_data_catalogs(\n",
    "    data_catalogs=[f\"{Path().absolute()}/samples/pangeo-cmip6.json\"],\n",
    "    variables_and_freqs=variables_and_freqs,\n",
    "    other_search_criteria=other_search_criteria,\n",
    "    match_hist_and_fut=True,\n",
    "    restrict_resolution=\"finest\",\n",
    ")\n",
    "\n",
    "cat_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82535e6c",
   "metadata": {},
   "source": [
    "If required, at this stage a dataset can be looked at in more details. If we examine the results (look at the 'date_start' and 'date_end' columns), we'll see that it successfully found historical simulations in the *CMIP* activity and renamed both their *activity* and *experiment* to match the future simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e5bd7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_sim[\"ScenarioMIP_NOAA-GFDL_GFDL-CM4_ssp585_r1i1p1f1_gr1\"].df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bddc58d",
   "metadata": {},
   "source": [
    "#### Example 2: Advanced search\n",
    "\n",
    "`allow_resampling` and `allow_conversion` are powerful search tools to find data that doesn't explicitely exist in the catalog, but that can easily be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30951b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_sim_adv = xs.search_data_catalogs(\n",
    "    data_catalogs=[f\"{Path().absolute()}/samples/pangeo-cmip6.json\"],\n",
    "    variables_and_freqs={\"evspsblpot\": \"D\", \"tas\": \"YS\"},\n",
    "    other_search_criteria={\"source\": [\"NorESM2-MM\"], \"processing_level\": [\"raw\"]},\n",
    "    match_hist_and_fut=True,\n",
    "    allow_resampling=True,\n",
    "    allow_conversion=True,\n",
    ")\n",
    "cat_sim_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b2ad7",
   "metadata": {},
   "source": [
    "If we examine the SSP5-8.5 results, we'll see that while it failed to find *evspsblpot*, it successfully understood that *tasmin* and *tasmax* can be used to compute it. It also understood that daily *tas* is a valid search result for `{tas: YS}`, since it can be aggregated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2d3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_sim_adv[\"ScenarioMIP_NCC_NorESM2-MM_ssp585_r1i1p1f1_gn\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962a3c22-58f4-49c4-94ff-78d9c10bcc85",
   "metadata": {},
   "source": [
    "It's also possible to search for multiple frequencies at the same time by using a list of xrfreq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f79d33d-4687-416a-91e6-d4f37ea5efef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_sim_adv_multifreq = xs.search_data_catalogs(\n",
    "    data_catalogs=[f\"{Path().absolute()}/samples/pangeo-cmip6.json\"],\n",
    "    variables_and_freqs={\"tas\": [\"D\", \"MS\", \"YS\"]},\n",
    "    other_search_criteria={\n",
    "        \"source\": [\"NorESM2-MM\"],\n",
    "        \"processing_level\": [\"raw\"],\n",
    "        \"experiment\": [\"ssp370\"],\n",
    "    },\n",
    "    match_hist_and_fut=True,\n",
    "    allow_resampling=True,\n",
    "    allow_conversion=True,\n",
    ")\n",
    "print(\n",
    "    cat_sim_adv_multifreq[\n",
    "        \"ScenarioMIP_NCC_NorESM2-MM_ssp370_r1i1p1f1_gn\"\n",
    "    ]._requested_variable_freqs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b7350a",
   "metadata": {},
   "source": [
    "#### Derived variables\n",
    "\n",
    "The `allow_conversion` argument is built upon `xclim`'s virtual indicators module and `intake-esm`'s [DerivedVariableRegistry](https://ncar.github.io/esds/posts/2021/intake-esm-derived-variables/) in a way that should be seamless to the user. It works by using the methods defined in `xscen/xclim_modules/conversions.yml` to add a registry of *derived* variables that exist virtually through computation methods.\n",
    "\n",
    "In the example above, we can see that the search failed to find *evspsblpot* within *NORESM2-MM*, but understood that *tasmin* and *tasmax* could be used to estimate it using `xclim`'s `potential_evapotranspiration`.\n",
    "\n",
    "Most use cases should already be covered by the aforementioned file. The preferred way to add new methods is to [submit a new indicator to xclim](https://xclim.readthedocs.io/en/stable/contributing.html), and then to add a call to that indicator in `conversions.yml`. In the case where this is not possible or where the transformation would be out of scope for `xclim`, the calculation can be implemented into `xscen/xclim_modules/conversions.py` instead.\n",
    "\n",
    "Alternatively, if other functions or other parameters are required for a specific use case (e.g. using `relative_humidity` instead of `relative_humidity_from_dewpoint`, or using a different formula), then a custom YAML file can be used. This custom file can be referred to using the `conversion_yaml` argument of `search_data_catalogs`.\n",
    "\n",
    "`.derivedcat` can be called on a catalog to obtain the list of DerivedVariable and the function associated to them. In addition, `._requested_variables` will display the list of variables that will be opened by the `to_dataset_dict()` function, including *DerivedVariables*.\n",
    "\n",
    "**NOTE:** `_requested_variables` should NOT be modified under any circumstance, as it is likely to make `to_dataset_dict()` fail. To add some transparency on which variables have been *requested* and which are the *dependent* ones, `xscen` has added `_requested_variables_true` and `_dependent_variables`. This is very likely to be changed in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee75ffb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_sim_adv[\"ScenarioMIP_NCC_NorESM2-MM_ssp585_r1i1p1f1_gn\"].derivedcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c235a0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cat_sim_adv[\"ScenarioMIP_NCC_NorESM2-MM_ssp585_r1i1p1f1_gn\"]._requested_variables)\n",
    "print(\n",
    "    f\"Requested: {cat_sim_adv['ScenarioMIP_NCC_NorESM2-MM_ssp585_r1i1p1f1_gn']._requested_variables_true}\"\n",
    ")\n",
    "print(\n",
    "    f\"Dependent: {cat_sim_adv['ScenarioMIP_NCC_NorESM2-MM_ssp585_r1i1p1f1_gn']._dependent_variables}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec5c3fc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"> <b>WARNING:</b> Note that `allow_conversion`  currently fails if:\n",
    "<ul>\n",
    "<li>The requested DerivedVariable also requires a DerivedVariable itself.</li>\n",
    "<li>The dependent variables exist at different frequencies (e.g. 'pr @1hr' & 'tas @3hr')</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d80ca03",
   "metadata": {},
   "source": [
    "## Creating a New Catalog from a Directory\n",
    "\n",
    "### Initialisation\n",
    "\n",
    "The `create` argument of `ProjectCatalog` can be called to create an empty *ProjectCatalog* and a new set of JSON and CSV files. The JSON file follows the ESM Catalog Specification v.0.1.0: https://github.com/NCAR/esm-collection-spec\n",
    "\n",
    "By default, `xscen` will populate the JSON with generic information, defined in `catalog.esm_col_data`. That metadata can be changed using the `project` argument with entries compatible with the ESM Catalog Specification (refer to the link above). Usually, the most useful and common entries will be: \n",
    "\n",
    "- title\n",
    "- description\n",
    "\n",
    "`xscen` will also instruct `intake_esm` to group catalog lines per *id - domain - processing_level - xrfreq*. This should be adequate for most uses. In the case that it is not, the following can be added to `project`:\n",
    "\n",
    "- \"aggregation_control\": {\"groupby_attrs\": [list_of_columns]}\n",
    "\n",
    "Other attributes and behaviours of the project definition can be modified in a similar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a0e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = {\n",
    "    \"title\": \"tutorial-catalog\",\n",
    "    \"description\": \"Catalog for the tutorial NetCDFs.\",\n",
    "}\n",
    "\n",
    "PC = ProjectCatalog(\n",
    "    str(output_folder / \"tutorial-catalog.json\"),\n",
    "    create=True,\n",
    "    project=project,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The metadata is stored in PC.esmcat\n",
    "PC.esmcat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9513fc05",
   "metadata": {},
   "source": [
    "### Appending new data to a ProjectCatalog\n",
    "\n",
    "At this stage, the CSV is still empty. There are two main ways to populate a catalog with data:\n",
    "\n",
    "- Using `xs.ProjectCatalog.update_from_ds` to append a Dataset and populate the catalog columns using metadata. \n",
    "\n",
    "- Using `xs.catalog.parse_directory` to parse through existing NetCDF or Zarr data and decode their information based on file and directory names.\n",
    "\n",
    "This tutorial will focus on `catalog.parse_directory`, as `update_from_ds` is moreso a function that will be called during a climate-scenario-generation workflow. See the [Getting Started](2_getting_started.ipynb#Updating-the-catalog) tutorial for more details on `update_from_ds`.\n",
    "\n",
    "#### Parsing a directory \n",
    "\n",
    "<div class=\"alert alert-info\"> <b>NOTE:</b> If you are an Ouranos employee, this section should be of limited use (unless you need to retroactively parse a directory containing exiting datasets). Please consult the existing Ouranos catalogs using xs.search_data_catalogs instead.</div>\n",
    "\n",
    "The `parse_directory` function relies on analyzing patterns to adequately decode the filenames to store that information in the catalog. \n",
    "\n",
    "- Patterns are a succession of column names in curly brackets. See below for an example. The pattern starts where the directory path stops.\n",
    "- The `parallel_depth` argument can be used to change the level at which to parallelize the file (and thus the `globpattern`) search. A value of 1 (default and minimum), means the subfolders of each directory are searched in parallel, a value of 2 would search the subfolders' subfolders in parallel, and so on.\n",
    "- If necessary, `read_from_file` can be used to open the files and read metadata from global attributes. Refer to the API for Docstrings and usage.\n",
    "- In cases where some column information is the same across all data, `homogenous_info` can be used to explicitely give an attribute to the datasets being processed.\n",
    "- Anything that isn't filled will be marked as `None`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xscen.catalog import parse_directory\n",
    "\n",
    "df = parse_directory(\n",
    "    directories=[\"samples/tutorial/\"],\n",
    "    globpattern=\"*.nc\",\n",
    "    patterns=[\n",
    "        \"{activity}/{domain}/{institution}/{source}/{experiment}/{member}/{frequency}/{*}.nc\"\n",
    "    ],\n",
    "    parallel_depth=1,\n",
    "    homogenous_info={\n",
    "        \"mip_era\": \"CMIP6\",\n",
    "        \"type\": \"simulation\",\n",
    "        \"processing_level\": \"raw\",\n",
    "    },\n",
    "    read_from_file=[\"variable\", \"date_start\", \"date_end\"],\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3b9414",
   "metadata": {},
   "source": [
    "#### Unique Dataset ID\n",
    "\n",
    "In addition to the parse itself, `catalog.parse_directory` will create a unique Dataset ID that can be used to easily determine one simulation from another. This can be edited with the `id_columns` argument of `parse_directory`, but by default, IDs are based on CMIP6's ID structure with additions related to regional models and bias adjustment:\n",
    "\n",
    "- `{bias_adjust_project} _ {mip_era} _ {activity} _ {driving_model} _ {institution} _ {source} _ {experiment} _ {member} _ {domain}`\n",
    "\n",
    "This utility can also be called by itself through `xs.catalog.generate_id()`.\n",
    "\n",
    "**NOTE:** Note that when constructing IDs, empty columns will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933fa4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0][\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d4e420",
   "metadata": {},
   "source": [
    "#### Appending data using ProjectCatalog.update()\n",
    "\n",
    "At this stage, `df` is a `pandas.DataFrame`. `ProjectCatalog.update` is used to append this data to the CSV file and save the results on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8954662",
   "metadata": {},
   "outputs": [],
   "source": [
    "PC.update(df)\n",
    "\n",
    "PC"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
